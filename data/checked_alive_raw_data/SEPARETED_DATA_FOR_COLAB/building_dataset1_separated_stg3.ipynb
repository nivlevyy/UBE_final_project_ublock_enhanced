{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 📥 הורדת Firefox ו-Geckodriver בצורה נכונה\n",
    "!apt-get update\n",
    "!apt install -y wget unzip\n",
    "!wget -q https://ftp.mozilla.org/pub/firefox/releases/115.0.2/linux-x86_64/en-US/firefox-115.0.2.tar.bz2\n",
    "!tar xjf firefox-115.0.2.tar.bz2\n",
    "!mv firefox /usr/local/firefox\n",
    "!ln -s /usr/local/firefox/firefox /usr/local/bin/firefox\n",
    "\n",
    "# 📥 הורדת Geckodriver\n",
    "!wget -q https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-linux64.tar.gz\n",
    "!tar -xvzf geckodriver-v0.36.0-linux64.tar.gz\n",
    "!chmod +x geckodriver\n",
    "!mv geckodriver /usr/local/bin/\n",
    "\n",
    "!pip install selenium\n",
    "!pip install tldextract\n",
    "!pip install bs4\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ9oj581RVUQ",
    "outputId": "6ce719bb-f5ba-4320-b57b-c0deae3665ae"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-26ubuntu3.2).\n",
      "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
      "mv: cannot overwrite non-directory '/usr/local/firefox/firefox' with directory 'firefox'\n",
      "ln: failed to create symbolic link '/usr/local/bin/firefox': File exists\n",
      "geckodriver\n",
      "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZSkcaSX-bcG",
    "outputId": "66a54712-981f-440c-e760-32be485d63da"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Created file: /content/test_urls_part_1.csv\n",
      "✅ Created file: /content/test_urls_part_2.csv\n",
      "✅ Created file: /content/test_urls_part_3.csv\n",
      "✅ Created file: /content/test_urls_part_4.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stage_3_config_dict = {\n",
    "        \"url_of_anchor_upper_tresh\":0.6,\n",
    "        \"url_of_anchor_lower_tresh\": 0.31,\n",
    "        \"link_count_html_upper_tresh\":0.81,\n",
    "        \"link_count_html_lower_tresh\": 0.13,\n",
    "        \"request_url_upper_tresh\":0.51,\n",
    "        \"iframe_upper_tresh\":6,\n",
    "        \"iframe_lower_tresh\": 2,\n",
    "        \"js_lower_tresh\":2,\n",
    "        \"js_upper_tresh\": 6,\n",
    "        \"nlp_upper_tresh\":0.008,\n",
    "        \"nlp_lower_tresh\":0.003,\n",
    "\n",
    "    }\n",
    "\n",
    "    #for java script sus behivor\n",
    "def get_high_risk_patterns():\n",
    "         return [\n",
    "                 r\"eval\\s*\\(\",\n",
    "                 r\"new\\s+Function\\s*\\(\",\n",
    "                 r\"document\\.write\\s*\\(\",\n",
    "                 r\"onmouseover\\s*=\",\n",
    "                 r\"setTimeout\\s*\\(\\s*['\\\"]\",\n",
    "             ]\n",
    "def get_medium_risk_patterns():\n",
    "     return [\n",
    "     r\"window\\.location\",\n",
    "     r\"innerHTML\\s*=\",\n",
    "     r\"onbeforeunload\",\n",
    " ]\n",
    "def get_low_risk_patterns():\n",
    "    return[\n",
    "     r\"navigator\\.clipboard\",\n",
    "     r\"XMLHttpRequest\",\n",
    "     r\"fetch\\s*\\(\"\n",
    " ]\n",
    "def get_known_safe_script_hosts():\n",
    "    return [\n",
    "     \"cdnjs.cloudflare.com\",\n",
    "     \"cdn.jsdelivr.net\",\n",
    "     \"ajax.googleapis.com\",\n",
    "     \"fonts.googleapis.com\",\n",
    "     \"fonts.gstatic.com\",\n",
    "     \"stackpath.bootstrapcdn.com\",\n",
    "     \"ajax.aspnetcdn.com\",\n",
    "     \"maxcdn.bootstrapcdn.com\",\n",
    "     \"code.jquery.com\",\n",
    "     \"cdn.jsdelivr.net\",\n",
    "     \"cdn.shopify.com\",\n",
    "     \"cdn.wix.com\",\n",
    "     \"unpkg.com\",\n",
    "     \"polyfill.io\",\n",
    "     \"bootstrapcdn.com\",\n",
    "     \"gstatic.com\",\n",
    "     \"google.com\",\n",
    "     \"googleapis.com\",\n",
    "     \"microsoft.com\",\n",
    "     \"cloudflare.com\",\n",
    "     \"cloudfront.net\",\n",
    "     \"fbcdn.net\",\n",
    "     \"facebook.com\",\n",
    "     \"yahooapis.com\",\n",
    "     \"notion.so\",\n",
    "     \"vercel.app\",\n",
    "     \"netlify.app\",\n",
    "     \"res.cloudinary.com\"\n",
    " ]\n",
    "\n",
    "def get_known_favicon_hosts():\n",
    "   return [\"google.com\", \"gstatic.com\", \"googleusercontent.com\", \"googleapis.com\", \"youtube.com\",\n",
    "                        \"ytimg.com\",\n",
    "                        \"apple.com\", \"microsoft.com\", \"office.com\", \"windows.com\", \"live.com\", \"microsoftonline.com\",\n",
    "                        \"adobe.com\", \"typekit.net\", \"adobestatic.com\", \"facebook.com\", \"fbcdn.net\", \"instagram.com\",\n",
    "                        \"cdninstagram.com\", \"twitter.com\", \"twimg.com\",\n",
    "                        \"linkedin.com\", \"licdn.com\", \"pinterest.com\", \"pinimg.com\", \"reddit.com\", \"redditstatic.com\",\n",
    "                        \"tumblr.com\", \"static.tumblr.com\",\n",
    "                        \"fonts.googleapis.com\", \"fonts.gstatic.com\", \"ajax.googleapis.com\",\n",
    "                        \"cloudflare.com\", \"cdnjs.cloudflare.com\", \"cdn.jsdelivr.net\",\n",
    "                        \"cdn.shopify.com\", \"stackpath.bootstrapcdn.com\", \"ajax.aspnetcdn.com\",\n",
    "                        \"akamaihd.net\", \"akamaized.net\", \"fastly.net\", \"cloudfront.net\", \"unpkg.com\",\n",
    "                        \"raw.githubusercontent.com\", \"github.com\", \"github.githubassets.com\",\n",
    "                        \"wp.com\", \"i0.wp.com\", \"i1.wp.com\", \"i2.wp.com\",\n",
    "                        \"squarespace.com\", \"squarespace-cdn.com\", \"static1.squarespace.com\", \"shopify.com\",\n",
    "                        \"cdn.shopify.com\", \"wix.com\", \"wixstatic.com\",\n",
    "                        \"paypal.com\", \"paypalobjects.com\", \"ebay.com\", \"ebaystatic.com\",\n",
    "                        \"amazon.com\", \"amazonaws.com\",\n",
    "                        \"yahoo.com\", \"yimg.com\", \"yahooapis.com\", \"bootstrapcdn.com\", \"maxcdn.bootstrapcdn.com\",\n",
    "                        \"jsdelivr.net\", \"fastly.com\",\n",
    "                        \"googletagmanager.com\", \"googlesyndication.com\", \"doubleclick.net\", \"googledomains.com\",\n",
    "                        \"firebaseio.com\", \"firebaseapp.com\", \"notion.so\", \"notion-static.com\",\n",
    "                        \"netlify.app\", \"vercel.app\", \"cloudinary.com\", \"res.cloudinary.com\"\n",
    "                        ]\n",
    "def get_suspicious_keywords():\n",
    "     return [\"login\", \"signin\", \"verify\", \"auth\", \"password\", \"2fa\", \"secure\"]\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from tldextract import extract\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "from re import error\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from tldextract import extract\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import logging\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from multiprocessing import Process\n",
    "\n",
    "SUSPICIOUS_WORDS_REGEX = re.compile(\n",
    "    r\"(log[\\s\\-]?in|sign[\\s\\-]?in|auth|user(name)?|email|phone|account|\"\n",
    "    r\"credential|password|passcode|pin|security[\\s\\-]?code|credit[\\s\\-]?card|cvv|expiry|iban|bank)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "lEGIT=1\n",
    "sUS=0\n",
    "pHISHING=-1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# הדאטה שלך\n",
    "data = {\n",
    "    \"URL\": [\n",
    "        \"https://heatmap.com\", \"https://gamepedia.com\",\n",
    "        \"https://9xflix.pink\", \"https://redditblog.com\"\n",
    "    ],\n",
    "    \"validity\": ['safe', 'safe', 'safe', 'safe']\n",
    "}\n",
    "\n",
    "# יוצרים DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# מחלקים ל-4 חלקים שווים (בערך)\n",
    "splits = np.array_split(df, 4)\n",
    "\n",
    "# שומרים כל חלק לקובץ נפרד\n",
    "for idx, split_df in enumerate(splits, start=1):\n",
    "    output_path = f\"/content/test_urls_part_{idx}.csv\"\n",
    "    split_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Created file: {output_path}\")\n",
    "\n",
    "# מציג את הראשון כדוגמה\n",
    "splits[0].head()\n",
    "def safe_extract(tag, attribute):\n",
    "    try:\n",
    "        return tag.get(attribute,\"\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def element_extraction_from_html(soup_html:BeautifulSoup,tag:str=None,attribute=None)->list:\n",
    "   #generic function to extract <tag> and its attribute like <link> \"href\"\n",
    "    if not tag:\n",
    "        return []\n",
    "    if attribute:\n",
    "       #change to this ->> soup_html.find_all(tag, atribute=True)\n",
    "        return soup_html.find_all(tag,**{attribute:True})\n",
    "    else:\n",
    "        return soup_html.find_all(tag)\n",
    "\n",
    "#done\n",
    "\n",
    "def normalize_domain(url:str):\n",
    "    parts_of_url = extract(url)\n",
    "    if not (parts_of_url.domain and parts_of_url.suffix):\n",
    "        return None\n",
    "    domain=f\"{parts_of_url.domain}.{parts_of_url.suffix}\"\n",
    "    return domain\n",
    "''' specific  function: check the html element that competable with each tag that indicates for a phishing site '''\n",
    "\n",
    "\n",
    "\n",
    "#####################1-favicon#############################\n",
    "\n",
    "def has_icon_func(icon_links:list)->int:\n",
    "    has_icon= 1 if len(icon_links) > 0 else 0\n",
    "    return has_icon\n",
    "\n",
    "def favicon_check(link_tag:list, base_domain:str) -> tuple:\n",
    "\n",
    "    icon_links = []\n",
    "    expected_domain = base_domain.lower()\n",
    "    favicon_domain_not_the_same=0\n",
    "    favicon_endwith=0\n",
    "    #filter only link tags related to favicon\n",
    "    for link in link_tag:\n",
    "        rel = link.get(\"rel\", []) #return all relations to the page (rel=\"something) is in the element <link>)\n",
    "        if any(\"icon\" in r.lower() for r in rel):\n",
    "            icon_links.append(link)\n",
    "    has_icon = has_icon_func(icon_links)\n",
    "\n",
    "    for link in icon_links:\n",
    "        href = link[\"href\"]\n",
    "        parsed =normalize_domain(href)\n",
    "\n",
    "        if not parsed:\n",
    "            continue\n",
    "\n",
    "        domain = parsed.lower()\n",
    "        if any(safe in domain for safe in get_known_favicon_hosts()):\n",
    "            continue\n",
    "\n",
    "        if not (domain == expected_domain):\n",
    "            favicon_domain_not_the_same+=1\n",
    "\n",
    "        # Check for acceptable file extensions in the requested url path.\n",
    "        if not (href.endswith(\".ico\") or href.endswith(\".png\") or href.endswith(\".gif\")):\n",
    "            favicon_endwith+=1\n",
    "\n",
    "    return has_icon,favicon_domain_not_the_same,favicon_endwith\n",
    "\n",
    "\n",
    "#####################2-anchor#############################\n",
    "# <a> tag ,attr - href=\n",
    "\n",
    "def extract_url_of_anchor_feature(a_list : list,base_domain) -> tuple:\n",
    "\n",
    "    total = int(len(a_list))\n",
    "    anchor_tags_count = 1 if total>0 else 0\n",
    "    anchor_empty_href_count = 0\n",
    "    anchor_domain_not_the_same = 0\n",
    "\n",
    "    for tag in a_list:\n",
    "        href =safe_extract(tag,\"href\")\n",
    "\n",
    "        if (not href) or href.strip() in [\"#\", \"javascript:void(0);\", \"javascript:\"]:\n",
    "            anchor_empty_href_count += 1\n",
    "            continue\n",
    "\n",
    "        href_domain = normalize_domain(href)\n",
    "        if not href_domain:\n",
    "            continue\n",
    "        if  href_domain != base_domain:\n",
    "            anchor_domain_not_the_same += 1\n",
    "\n",
    "    return anchor_tags_count,anchor_empty_href_count,anchor_domain_not_the_same\n",
    "\n",
    "\n",
    "#####################3-link_count#############################\n",
    "# <script> attr-> src =,<meta> attr - content,<links> tag ,attr - href=\n",
    "#here i divided the big function into 4 smaller ones for each component\n",
    "def count_external_script_src(script_list:list, base_domain: str)->tuple:\n",
    "    external_script_count=0\n",
    "    sus_words_in_script=0\n",
    "\n",
    "    for tag in script_list:\n",
    "        value=safe_extract(tag, \"src\")\n",
    "        if not value:\n",
    "            continue\n",
    "        domain = normalize_domain(value)\n",
    "        if not domain:\n",
    "            continue\n",
    "        if domain != base_domain:\n",
    "            external_script_count += 1\n",
    "        text = \" \".join((tag.string or \"\"))\n",
    "        if not text:\n",
    "            continue\n",
    "        matches = SUSPICIOUS_WORDS_REGEX.findall(text.lower())\n",
    "        sus_words_in_script +=len(matches)\n",
    "\n",
    "    return external_script_count , sus_words_in_script\n",
    "\n",
    "def count_external_meta_content(meta_list: list, base_domain: str) -> tuple:\n",
    "    external_meta_count = 0\n",
    "    sus_words_in_meta=0\n",
    "    for tag in meta_list:\n",
    "        value = safe_extract(tag, \"content\")\n",
    "        if not value:\n",
    "            continue\n",
    "        domain = normalize_domain(value)\n",
    "        if not domain:\n",
    "            continue\n",
    "        if domain != base_domain:\n",
    "            external_meta_count += 1\n",
    "        text = \" \".join(value)\n",
    "        if not text:\n",
    "            continue\n",
    "        matches = SUSPICIOUS_WORDS_REGEX.findall(text.lower())\n",
    "        sus_words_in_meta += len(matches)\n",
    "    return external_meta_count,sus_words_in_meta\n",
    "\n",
    "\n",
    "def count_external_link_href(link_list: list, base_domain: str) -> int:\n",
    "    external_link_count = 0\n",
    "    for tag in link_list:\n",
    "        value = safe_extract(tag, \"src\")\n",
    "        if not value:\n",
    "            continue\n",
    "        domain = normalize_domain(value)\n",
    "        if not domain:\n",
    "            continue\n",
    "        if domain != base_domain:\n",
    "            external_link_count += 1\n",
    "\n",
    "    return external_link_count\n",
    "\n",
    "\n",
    "def link_count_in_html(extern_links:int,extern_meta:int,extern_script:int) -> int:\n",
    "    total_extern_links=extern_links +extern_meta +extern_script\n",
    "    return total_extern_links\n",
    "\n",
    "\n",
    "#####################4-request_url#############################\n",
    "# tags--->    < img,source,audio,video,embed,iframe >, attr-> src\n",
    "\n",
    "def extract_request_url_feature(resources_elements_list: list, base_domain: str) -> tuple:\n",
    "    total_resources = len(resources_elements_list)\n",
    "    external_count = 0\n",
    "\n",
    "    for tag in resources_elements_list:\n",
    "        src = safe_extract(tag, \"src\")\n",
    "        if not src:\n",
    "            continue\n",
    "        domain = normalize_domain(src)\n",
    "        if not domain:\n",
    "            continue\n",
    "        if domain != base_domain:\n",
    "            external_count += 1\n",
    "\n",
    "    return total_resources, external_count\n",
    "\n",
    "\n",
    "#####################5-sfh#############################\n",
    "# <form> tag ,attr - action/nothing=\n",
    "\n",
    "def extract_sfh_feature(form_list: list, base_domain: str) -> tuple:\n",
    "    sfh_count=len(form_list)\n",
    "    sfh_action_is_blank = 0\n",
    "    sfh_domain_not_the_same=0\n",
    "    password_in_sfh=0\n",
    "    has_suspicious_words=0\n",
    "    for form in form_list:\n",
    "        action = safe_extract(form, \"action\")\n",
    "        if not action or action.lower() in [\"about:blank\", \"#\", \"\"]:\n",
    "            sfh_action_is_blank += 1\n",
    "        else:\n",
    "            action_domain = normalize_domain(action)\n",
    "            if action_domain and action_domain != base_domain:\n",
    "                sfh_domain_not_the_same += 1\n",
    "\n",
    "        inputs = form.find_all(\"input\")\n",
    "        for i in inputs:\n",
    "            t = i.get(\"type\", \"\").lower()\n",
    "            n = i.get(\"name\", \"\").lower()\n",
    "            if t == \"password\":\n",
    "                password_in_sfh += 1\n",
    "            if any(w in n for w in get_suspicious_keywords()):\n",
    "                has_suspicious_words += 1\n",
    "\n",
    "    return  sfh_count,sfh_action_is_blank,sfh_domain_not_the_same,password_in_sfh,has_suspicious_words\n",
    "\n",
    "#####################6-iframe#############################\n",
    "# <iframe> tag ,attr - src/srcdoc=\n",
    "\n",
    "def extract_iframe_feature_src(frame_src_list: list, base_domain: str) -> tuple:\n",
    "\n",
    "    iframe_src_count=0\n",
    "    iframe_src_style_hidden=0\n",
    "    iframe_src_size=0\n",
    "    iframe_src_domain_not_the_same=0\n",
    "    iframe_no_src_sendbox=0\n",
    "    try:\n",
    "        for iframe in frame_src_list:\n",
    "\n",
    "            src = iframe.get(\"src\", \"\").strip().lower()\n",
    "            if src:\n",
    "                iframe_src_count+=1\n",
    "\n",
    "            style = iframe.get(\"style\", \"\").lower()\n",
    "            width = iframe.get(\"width\", \"\").strip()\n",
    "            height = iframe.get(\"height\", \"\").strip()\n",
    "            iframe_domain = normalize_domain(src) if src else \"\"\n",
    "\n",
    "            is_external = iframe_domain and iframe_domain != base_domain\n",
    "\n",
    "            if any(x in src for x in [\"ads\", \"analytics\", \"pixel\", \"tracker\", \"doubleclick\"]):\n",
    "                continue\n",
    "\n",
    "            if \"display:none\" in style or \"visibility:hidden\" in style:\n",
    "                iframe_src_style_hidden += 1\n",
    "\n",
    "            if width == \"0\" or height == \"0\":\n",
    "                iframe_src_size += 1\n",
    "\n",
    "            if is_external:\n",
    "                iframe_src_domain_not_the_same += 1\n",
    "\n",
    "            if not iframe.has_attr(\"sandbox\"):\n",
    "                iframe_no_src_sendbox += 1\n",
    "    except Exception as e:\n",
    "        return 0,0,0,0,0\n",
    "    return   iframe_src_count, iframe_src_style_hidden,  iframe_src_size, iframe_src_domain_not_the_same, iframe_no_src_sendbox\n",
    "\n",
    "def extract_iframe_feature_srcdoc(iframe_list: list, base_domain: str) -> tuple:\n",
    "    iframe_srcdoc_count = 0\n",
    "    iframe_src_doc_hidden = 0\n",
    "    iframe_srcdoc_js_existence = 0\n",
    "    iframe_srcdoc_sus_words= 0\n",
    "\n",
    "    try:\n",
    "        for iframe in iframe_list:\n",
    "\n",
    "            srcdoc = iframe.get(\"srcdoc\", \"\").strip().lower()\n",
    "\n",
    "            if srcdoc:\n",
    "                iframe_srcdoc_count+=1\n",
    "                clean_srcdoc_text = BeautifulSoup(srcdoc, \"html.parser\").get_text().lower()\n",
    "\n",
    "                if SUSPICIOUS_WORDS_REGEX.search(clean_srcdoc_text):\n",
    "                    iframe_srcdoc_sus_words += 1\n",
    "                if \"<script\" in srcdoc or \"javascript:\" in srcdoc:\n",
    "                    iframe_srcdoc_js_existence += 1\n",
    "                if \"display:none\" in srcdoc or \"visibility:hidden\" in srcdoc:\n",
    "                    iframe_src_doc_hidden += 1\n",
    "\n",
    "        return iframe_srcdoc_count,  iframe_src_doc_hidden,   iframe_srcdoc_js_existence , iframe_srcdoc_sus_words\n",
    "    except Exception as e:\n",
    "        return iframe_srcdoc_count,  iframe_src_doc_hidden,   iframe_srcdoc_js_existence , iframe_srcdoc_sus_words\n",
    "\n",
    "def total_iframe_src_n_doc(src_count:int,srcdoc_count:int)->int:\n",
    "    return src_count+srcdoc_count\n",
    "\n",
    "#####################7-suspicious_js#############################\n",
    "\n",
    "def detect_suspicious_js_behavior(soup: BeautifulSoup, base_domain: str) -> tuple:\n",
    "    score = 0\n",
    "    inline_scripts_count=0\n",
    "    high_risk_patterns_count=0\n",
    "    medium_risk_patterns_count=0\n",
    "    low_risk_patterns_count=0\n",
    "    sus_js_domain_not_the_same=0\n",
    "\n",
    "    try:\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "    except Exception:\n",
    "        return inline_scripts_count,high_risk_patterns_count,medium_risk_patterns_count,low_risk_patterns_count,sus_js_domain_not_the_same\n",
    "    inline_scripts_count=len(inline_scripts)\n",
    "\n",
    "    for script in inline_scripts:\n",
    "        try:\n",
    "            content = script.get_text().strip().lower()\n",
    "        except Exception:\n",
    "            content = \"\"\n",
    "\n",
    "        for pattern in get_high_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                high_risk_patterns_count += 1\n",
    "\n",
    "        for pattern in get_medium_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                medium_risk_patterns_count += 1\n",
    "\n",
    "        for pattern in get_low_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                low_risk_patterns_count += 1\n",
    "\n",
    "    try:\n",
    "        external_scripts = soup.find_all(\"script\", src=True)\n",
    "        for script in external_scripts:\n",
    "            src = safe_extract(script, \"src\")\n",
    "            domain = normalize_domain(src)\n",
    "            if domain and domain != base_domain and domain not in get_known_safe_script_hosts():\n",
    "                sus_js_domain_not_the_same += 1\n",
    "\n",
    "        return inline_scripts_count,    high_risk_patterns_count,    medium_risk_patterns_count,    low_risk_patterns_count,   sus_js_domain_not_the_same\n",
    "\n",
    "    except Exception:\n",
    "        return inline_scripts_count,    high_risk_patterns_count,    medium_risk_patterns_count,    low_risk_patterns_count,   sus_js_domain_not_the_same\n",
    "\n",
    "\n",
    "#####################8-nlp#############################\n",
    "\n",
    "def nlp_based_phishing_text_check(soup: BeautifulSoup) -> int:\n",
    "    text = soup.get_text(strip=True).lower()\n",
    "\n",
    "    matches = SUSPICIOUS_WORDS_REGEX.findall(text)\n",
    "\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "#####################9-analyze_textual_tags#############################\n",
    "\n",
    "def analyze_textual_tags(count_script:int,count_meta:int) -> int:\n",
    "    return count_script + count_meta\n",
    "\n",
    "\n",
    "##### to stage 3 full loaded html\n",
    "####################10-dynamic_script#############################\n",
    "def detect_dynamic_script_injection(driver: webdriver) -> int:\n",
    "    try:\n",
    "        injected_scripts = driver.execute_script(\"\"\"\n",
    "            return [...document.scripts].filter(s => s.src || s.innerText.length > 0).length;\n",
    "        \"\"\")\n",
    "        return injected_scripts\n",
    "    except Exception as e:\n",
    "        return sUS\n",
    "\n",
    "#####################11-auto_redirect#############################\n",
    "############## after all html was loaded\n",
    "# this function is blocking because the waiting for full upload of the page ,\n",
    "def detect_auto_redirect(driver: webdriver, base_domain: str, timeout: float = 3.0) ->tuple:\n",
    "    meta_equiv=0\n",
    "    window_or_replace_redirect=0\n",
    "    autoredirect_different_domain=0\n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "\n",
    "        page_source = driver.page_source.lower()\n",
    "    except Exception:\n",
    "        return meta_equiv,  window_or_replace_redirect, autoredirect_different_domain\n",
    "    try:\n",
    "\n",
    "        if re.search(r'<meta\\s+http-equiv\\s*=\\s*[\"\\']?refresh[\"\\']?', page_source, re.IGNORECASE):\n",
    "            meta_equiv=1\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if re.search(r'(window\\.)?location\\.(href|replace)', page_source):\n",
    "            window_or_replace_redirect=1\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        final_url = driver.current_url\n",
    "\n",
    "        if normalize_domain(final_url) != base_domain:\n",
    "            autoredirect_different_domain=1\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return meta_equiv, window_or_replace_redirect, autoredirect_different_domain\n",
    "\n",
    "\n",
    "\n",
    "#####################12-login_form_visibility#############################\n",
    "def check_login_form_visibility(driver: webdriver) -> int:\n",
    "    hidden=0\n",
    "    try:\n",
    "        script = \"\"\"\n",
    "        var forms = document.getElementsByTagName('form');\n",
    "        for (var i = 0; i < forms.length; i++) {\n",
    "            var style = window.getComputedStyle(forms[i]);\n",
    "            if (style.display === 'none' || style.visibility === 'hidden' ||\n",
    "                forms[i].offsetWidth === 0 || forms[i].offsetHeight === 0) {\n",
    "                return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "        \"\"\"\n",
    "        hidden = driver.execute_script(script)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return int(hidden)\n",
    "\n",
    "#####################13-onmouseover#############################\n",
    "def detect_onmouseover_in_dom(soup: BeautifulSoup) -> tuple:\n",
    "    tags_with_onmouseover_count=0\n",
    "    suspicious_script_detected=0\n",
    "    try:\n",
    "        tags_with_onmouseover = soup.find_all(attrs={\"onmouseover\": True})\n",
    "        tags_with_onmouseover_count=  len(tags_with_onmouseover)\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "        suspicious_script_detected = sum(1 for script in inline_scripts if \"onmouseover\" in (script.string or \"\").lower())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return suspicious_script_detected,tags_with_onmouseover_count\n",
    "\n",
    "#####################14-right_click_block#############################\n",
    "### when checking in browser return to original function because it is very sus\n",
    "def detect_right_click_block(soup: BeautifulSoup) -> tuple:\n",
    "\n",
    "    suspicious_script_count=0\n",
    "    contextmenu_tags_count=0\n",
    "    try:\n",
    "        contextmenu_tags = soup.find_all(attrs={\"oncontextmenu\": True})\n",
    "        contextmenu_tags_count=len(contextmenu_tags)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "        for script in inline_scripts:\n",
    "            content = (script.string or \"\").lower()\n",
    "\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            if re.search(r\"event\\s*\\.\\s*button\\s*==\\s*2\", content) or \"contextmenu\" in content:\n",
    "                suspicious_script_count +=1\n",
    "    except Exception:\n",
    "        pass\n",
    "    return suspicious_script_count,contextmenu_tags_count\n",
    "\n",
    "##############################################end###################################\n",
    "\n",
    "\n",
    "def find_html_features_separated(soup: BeautifulSoup, url: str, feature_type: str, driver: webdriver):\n",
    "    domain = normalize_domain(url)\n",
    "    elements = []\n",
    "\n",
    "    if feature_type == \"favicon_check\":\n",
    "        elements = element_extraction_from_html(soup, tag=\"link\", attribute=\"href\")\n",
    "        return favicon_check(elements, domain)\n",
    "\n",
    "    elif feature_type == \"url_anchor\":\n",
    "        elements = element_extraction_from_html(soup, tag=\"a\", attribute=\"href\")\n",
    "        return extract_url_of_anchor_feature(elements, domain)\n",
    "\n",
    "    elif feature_type == \"links_in_tags\":\n",
    "        meta_elements = element_extraction_from_html(soup, tag=\"meta\", attribute=\"content\")\n",
    "        script_elements = element_extraction_from_html(soup, tag=\"script\", attribute=\"src\")\n",
    "        link_elements = element_extraction_from_html(soup, tag=\"link\", attribute=\"href\")\n",
    "        extern_meta, sus_words_meta = count_external_meta_content(meta_elements, domain)\n",
    "        extern_script, sus_words_script = count_external_script_src(script_elements, domain)\n",
    "        extern_links = count_external_link_href(link_elements, domain)\n",
    "        total_extern = link_count_in_html(extern_links, extern_meta, extern_script)\n",
    "        return extern_meta, sus_words_meta, extern_script, sus_words_script, extern_links, total_extern\n",
    "\n",
    "    elif feature_type == \"request_sources_from_diff_url\":\n",
    "        for tag in [\"img\", \"source\", \"audio\", \"video\", \"embed\", \"iframe\"]:\n",
    "            elements += element_extraction_from_html(soup, tag=tag, attribute=\"src\")\n",
    "        return extract_request_url_feature(elements, domain)\n",
    "\n",
    "    elif feature_type == \"sfh\":\n",
    "        elements = element_extraction_from_html(soup, tag=\"form\")\n",
    "        return extract_sfh_feature(elements, domain)\n",
    "\n",
    "    elif feature_type == \"iframe\":\n",
    "        iframe_elements = element_extraction_from_html(soup, tag=\"iframe\")\n",
    "        src_features = extract_iframe_feature_src(iframe_elements, domain)\n",
    "        srcdoc_features = extract_iframe_feature_srcdoc(iframe_elements, domain)\n",
    "        total_iframes = total_iframe_src_n_doc(src_features[0], srcdoc_features[0])\n",
    "        return src_features + srcdoc_features + (total_iframes,)\n",
    "\n",
    "    elif feature_type == \"suspicious_js\":\n",
    "        return detect_suspicious_js_behavior(soup, domain)\n",
    "\n",
    "    elif feature_type == \"nlp_text\":\n",
    "        return (nlp_based_phishing_text_check(soup),)\n",
    "\n",
    "    elif feature_type == \"analyze_textual_tags\":\n",
    "        meta_elements = element_extraction_from_html(soup, tag=\"meta\", attribute=\"content\")\n",
    "        script_elements = element_extraction_from_html(soup, tag=\"script\", attribute=\"src\")\n",
    "        count_meta, _ = count_external_meta_content(meta_elements, domain)\n",
    "        count_script, _ = count_external_script_src(script_elements, domain)\n",
    "        return (analyze_textual_tags(count_script, count_meta),)\n",
    "\n",
    "    elif feature_type == \"detect_dynamic_script_injection\":\n",
    "        return (detect_dynamic_script_injection(driver),)\n",
    "\n",
    "    elif feature_type == \"detect_auto_redirect\":\n",
    "        return detect_auto_redirect(driver, domain)\n",
    "\n",
    "    elif feature_type == \"check_login_form_visibility\":\n",
    "        return (check_login_form_visibility(driver),)\n",
    "\n",
    "    elif feature_type == \"detect_onmouseover_in_dom\":\n",
    "        return detect_onmouseover_in_dom(soup)\n",
    "\n",
    "    elif feature_type == \"detect_right_click_block\":\n",
    "        return detect_right_click_block(soup)\n",
    "\n",
    "    return ()\n",
    "def wait_for_initial_html(driver, min_length=500, timeout=5.0) -> str:\n",
    "    \"\"\"\n",
    "    Waits until there is enough initial HTML content.\n",
    "    Returns the HTML once ready, or the best available after timeout.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            ready_state = driver.execute_script(\"return document.readyState;\")\n",
    "            if ready_state not in [\"interactive\", \"complete\"]:\n",
    "                time.sleep(0.05)\n",
    "                continue\n",
    "\n",
    "            html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "            if html and len(html) >= min_length:\n",
    "                return html\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    # fallback\n",
    "    try:\n",
    "        html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    except Exception:\n",
    "        html = \"\"\n",
    "    return html\n",
    "\n",
    "def test_headless_browser_firefox(url: str, mode: str = \"full\") -> tuple:\n",
    "    \"\"\"\n",
    "    mode: \"initial\" -> fetch initial enough HTML\n",
    "          \"full\"    -> wait for fully loaded HTML\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "\n",
    "        ffx_options = Options()\n",
    "        ffx_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Firefox(options=ffx_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        if mode == \"full\":\n",
    "\n",
    "            # 🔵 מצב FULL → לחכות שהדף יטען לגמרי\n",
    "            WebDriverWait(driver, 10).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            html = driver.page_source\n",
    "        elif mode == \"initial\":\n",
    "            # 🟢 מצב INITIAL → לחכות שיהיה מספיק HTML ראשוני\n",
    "            html = wait_for_initial_html(driver, min_length=500, timeout=5.0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode. Use 'initial' or 'full'.\")\n",
    "\n",
    "        if not html or html.strip() == \"\":\n",
    "            logging.error(f\"[ERROR] Empty page source for {url}\")\n",
    "            return None, None\n",
    "\n",
    "        return html, driver\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ [ERROR] Failed to load {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    finally:\n",
    "        if driver is not None:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "# Set logging level to DEBUG\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "def extract_stage3_features_debug_separated(input_csv_path, output_csv_path,pid):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    total = len(df)\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['URL']\n",
    "        label = row.get('label', 0)\n",
    "        print(f\"proc id:{pid}\\n🔍 [{index+1}/{total}] Processing URL: {url}\")\n",
    "\n",
    "        html, driver = test_headless_browser_firefox(url,\"full\")    #change here to \"full\" or \"initial\"\n",
    "        if html is None or driver is None:\n",
    "            logging.error(f\"[ERROR] Failed to load page: {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            features = [url]\n",
    "\n",
    "            feature_types = [\n",
    "                \"favicon_check\",\n",
    "                \"url_anchor\",\n",
    "                \"links_in_tags\",\n",
    "                \"request_sources_from_diff_url\",\n",
    "                \"sfh\",\n",
    "                \"iframe\",\n",
    "                \"suspicious_js\",\n",
    "                \"nlp_text\",\n",
    "                \"analyze_textual_tags\",\n",
    "                \"detect_dynamic_script_injection\",\n",
    "                \"detect_auto_redirect\",\n",
    "                \"check_login_form_visibility\",\n",
    "                \"detect_onmouseover_in_dom\",\n",
    "                \"detect_right_click_block\"\n",
    "            ]\n",
    "\n",
    "            for feature_type in feature_types:\n",
    "                try:\n",
    "                    result = find_html_features_separated(soup, url, feature_type, driver)\n",
    "                    features.extend(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"[ERROR] {feature_type} failed for {url} → {e}\")\n",
    "                    features.extend([-999])\n",
    "\n",
    "            features.append(label)\n",
    "            results.append(features)\n",
    "        finally:\n",
    "            if driver is not None:\n",
    "                 try:\n",
    "                     driver.quit()\n",
    "                 except Exception:\n",
    "                     pass\n",
    "\n",
    "    headers = [\n",
    "        \"url\",\n",
    "        # favicon\n",
    "        \"has_icon\", \"favicon_diff_domain\", \"favicon_invalid_ext\",\n",
    "        # anchor\n",
    "        \"anchor_tags_present\", \"anchor_empty_href\", \"anchor_diff_domain\",\n",
    "        # links\n",
    "        \"meta_external\", \"meta_sus_words\", \"script_external\", \"script_sus_words\", \"link_external\", \"total_external\",\n",
    "        # request sources\n",
    "        \"total_resources\", \"external_resources\",\n",
    "        # sfh\n",
    "        \"sfh_total_forms\", \"sfh_blank_action\", \"sfh_diff_domain\", \"sfh_password_inputs\", \"sfh_suspicious_inputs\",\n",
    "        # iframe src\n",
    "        \"iframe_src_count\", \"iframe_src_hidden\", \"iframe_src_size\", \"iframe_src_diff_domain\", \"iframe_src_no_sandbox\",\n",
    "        # iframe srcdoc\n",
    "        \"iframe_srcdoc_count\", \"iframe_srcdoc_hidden\", \"iframe_srcdoc_scripts\", \"iframe_srcdoc_sus_words\",\n",
    "        # total iframes\n",
    "        \"total_iframes\",\n",
    "        # suspicious_js\n",
    "        \"inline_scripts\", \"high_risk_patterns\", \"medium_risk_patterns\", \"low_risk_patterns\", \"sus_js_diff_domain\",\n",
    "        # nlp\n",
    "        \"nlp_suspicious_words\",\n",
    "        # analyze_textual_tags\n",
    "        \"analyze_textual_sus_words\",\n",
    "        # dynamic script injection\n",
    "        \"dynamic_scripts_count\",\n",
    "        # auto redirect\n",
    "        \"meta_refresh_redirect\", \"window_location_redirect\", \"final_url_diff_domain\",\n",
    "        # hidden login forms\n",
    "        \"hidden_forms_count\",\n",
    "        # onmouseover\n",
    "        \"onmouseover_scripts\", \"onmouseover_tags\",\n",
    "        # right click block\n",
    "        \"right_click_scripts\", \"right_click_tags\",\n",
    "        # label\n",
    "        \"label\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    print(\"updated\")\n",
    "        # Save to CSV\n",
    "    pd.DataFrame(results, columns=headers).to_csv(output_csv_path, index=False)\n",
    "    logging.info(f\"\\n✅ Finished. CSV saved to: \\033[92m{output_csv_path}\\033[0m\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_single_file(input_csv_path, output_csv_path,pid):\n",
    "    try:\n",
    "        logging.info(f\"🚀 Starting pid= {pid} for {input_csv_path}\")\n",
    "        extract_stage3_features_debug_separated(input_csv_path, output_csv_path,pid)\n",
    "        logging.info(f\"✅ Finished pid={pid} for {input_csv_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Error in pid={pid} processing {input_csv_path}: {e}\")\n",
    "\n",
    "def main_parallel():\n",
    "\n",
    "    files = [\n",
    "        (\"/content/test_urls_part_1.csv\", \"/content/test1_urls_part_1.csv\",\"1\"),\n",
    "        (\"/content/test_urls_part_2.csv\", \"/content/test1_urls_part_2.csv\",\"2\"),\n",
    "        (\"/content/test_urls_part_3.csv\", \"/content/test1_urls_part_3.csv\",\"3\"),\n",
    "         (\"/content/test_urls_part_4.csv\", \"/content/test1_urls_part_4.csv\",\"4\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    total_files = len(files)\n",
    "    print(f\"🚀 Starting sequential processing of {total_files} files...\\n\")\n",
    "\n",
    "    for idx, (input_csv, output_csv, id) in enumerate(files, 1):\n",
    "        print(f\"🔵 [{idx}/{total_files}] Processing file ID={id}\")\n",
    "        try:\n",
    "            process_single_file(input_csv, output_csv, id)\n",
    "            print(f\"✅ Finished processing file ID={id}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing file ID={id}: {e}\\n\")\n",
    "\n",
    "    print(\"🎯 All files processed successfully!\\n\")\n",
    "\n",
    "main_parallel()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFLF0A00PLaP",
    "outputId": "80fabd08-caa3-4104-cd80-6d004e423698"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
