{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ğŸ“¥ ×”×•×¨×“×ª Firefox ×•-Geckodriver ×‘×¦×•×¨×” × ×›×•× ×”\n",
    "!apt-get update\n",
    "!apt install -y wget unzip\n",
    "!wget -q https://ftp.mozilla.org/pub/firefox/releases/115.0.2/linux-x86_64/en-US/firefox-115.0.2.tar.bz2\n",
    "!tar xjf firefox-115.0.2.tar.bz2\n",
    "!mv firefox /usr/local/firefox\n",
    "!ln -s /usr/local/firefox/firefox /usr/local/bin/firefox\n",
    "\n",
    "# ğŸ“¥ ×”×•×¨×“×ª Geckodriver\n",
    "!wget -q https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-linux64.tar.gz\n",
    "!tar -xvzf geckodriver-v0.36.0-linux64.tar.gz\n",
    "!chmod +x geckodriver\n",
    "!mv geckodriver /usr/local/bin/\n",
    "\n",
    "!pip install selenium\n",
    "!pip install tldextract\n",
    "!pip install bs4\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ9oj581RVUQ",
    "outputId": "adccb51a-0ef4-4ea1-9993-6e35582884b4",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "sZSkcaSX-bcG",
    "outputId": "938ea354-4702-4f10-9985-20419a5ee659"
   },
   "source": [
    "\n",
    "stage_3_config_dict = {\n",
    "        \"url_of_anchor_upper_tresh\":0.6,\n",
    "        \"url_of_anchor_lower_tresh\": 0.31,\n",
    "        \"link_count_html_upper_tresh\":0.81,\n",
    "        \"link_count_html_lower_tresh\": 0.13,\n",
    "        \"request_url_upper_tresh\":0.51,\n",
    "        \"iframe_upper_tresh\":6,\n",
    "        \"iframe_lower_tresh\": 2,\n",
    "        \"js_lower_tresh\":2,\n",
    "        \"js_upper_tresh\": 6,\n",
    "        \"nlp_upper_tresh\":0.008,\n",
    "        \"nlp_lower_tresh\":0.003,\n",
    "\n",
    "    }\n",
    "\n",
    "    #for java script sus behivor\n",
    "def get_high_risk_patterns():\n",
    "         return [\n",
    "                 r\"eval\\s*\\(\",\n",
    "                 r\"new\\s+Function\\s*\\(\",\n",
    "                 r\"document\\.write\\s*\\(\",\n",
    "                 r\"onmouseover\\s*=\",\n",
    "                 r\"setTimeout\\s*\\(\\s*['\\\"]\",\n",
    "             ]\n",
    "def get_medium_risk_patterns():\n",
    "     return [\n",
    "     r\"window\\.location\",\n",
    "     r\"innerHTML\\s*=\",\n",
    "     r\"onbeforeunload\",\n",
    " ]\n",
    "def get_low_risk_patterns():\n",
    "    return[\n",
    "     r\"navigator\\.clipboard\",\n",
    "     r\"XMLHttpRequest\",\n",
    "     r\"fetch\\s*\\(\"\n",
    " ]\n",
    "def get_known_safe_script_hosts():\n",
    "    return [\n",
    "     \"cdnjs.cloudflare.com\",\n",
    "     \"cdn.jsdelivr.net\",\n",
    "     \"ajax.googleapis.com\",\n",
    "     \"fonts.googleapis.com\",\n",
    "     \"fonts.gstatic.com\",\n",
    "     \"stackpath.bootstrapcdn.com\",\n",
    "     \"ajax.aspnetcdn.com\",\n",
    "     \"maxcdn.bootstrapcdn.com\",\n",
    "     \"code.jquery.com\",\n",
    "     \"cdn.jsdelivr.net\",\n",
    "     \"cdn.shopify.com\",\n",
    "     \"cdn.wix.com\",\n",
    "     \"unpkg.com\",\n",
    "     \"polyfill.io\",\n",
    "     \"bootstrapcdn.com\",\n",
    "     \"gstatic.com\",\n",
    "     \"google.com\",\n",
    "     \"googleapis.com\",\n",
    "     \"microsoft.com\",\n",
    "     \"cloudflare.com\",\n",
    "     \"cloudfront.net\",\n",
    "     \"fbcdn.net\",\n",
    "     \"facebook.com\",\n",
    "     \"yahooapis.com\",\n",
    "     \"notion.so\",\n",
    "     \"vercel.app\",\n",
    "     \"netlify.app\",\n",
    "     \"res.cloudinary.com\"\n",
    " ]\n",
    "\n",
    "def get_known_favicon_hosts():\n",
    "   return [\"google.com\", \"gstatic.com\", \"googleusercontent.com\", \"googleapis.com\", \"youtube.com\",\n",
    "                        \"ytimg.com\",\n",
    "                        \"apple.com\", \"microsoft.com\", \"office.com\", \"windows.com\", \"live.com\", \"microsoftonline.com\",\n",
    "                        \"adobe.com\", \"typekit.net\", \"adobestatic.com\", \"facebook.com\", \"fbcdn.net\", \"instagram.com\",\n",
    "                        \"cdninstagram.com\", \"twitter.com\", \"twimg.com\",\n",
    "                        \"linkedin.com\", \"licdn.com\", \"pinterest.com\", \"pinimg.com\", \"reddit.com\", \"redditstatic.com\",\n",
    "                        \"tumblr.com\", \"static.tumblr.com\",\n",
    "                        \"fonts.googleapis.com\", \"fonts.gstatic.com\", \"ajax.googleapis.com\",\n",
    "                        \"cloudflare.com\", \"cdnjs.cloudflare.com\", \"cdn.jsdelivr.net\",\n",
    "                        \"cdn.shopify.com\", \"stackpath.bootstrapcdn.com\", \"ajax.aspnetcdn.com\",\n",
    "                        \"akamaihd.net\", \"akamaized.net\", \"fastly.net\", \"cloudfront.net\", \"unpkg.com\",\n",
    "                        \"raw.githubusercontent.com\", \"github.com\", \"github.githubassets.com\",\n",
    "                        \"wp.com\", \"i0.wp.com\", \"i1.wp.com\", \"i2.wp.com\",\n",
    "                        \"squarespace.com\", \"squarespace-cdn.com\", \"static1.squarespace.com\", \"shopify.com\",\n",
    "                        \"cdn.shopify.com\", \"wix.com\", \"wixstatic.com\",\n",
    "                        \"paypal.com\", \"paypalobjects.com\", \"ebay.com\", \"ebaystatic.com\",\n",
    "                        \"amazon.com\", \"amazonaws.com\",\n",
    "                        \"yahoo.com\", \"yimg.com\", \"yahooapis.com\", \"bootstrapcdn.com\", \"maxcdn.bootstrapcdn.com\",\n",
    "                        \"jsdelivr.net\", \"fastly.com\",\n",
    "                        \"googletagmanager.com\", \"googlesyndication.com\", \"doubleclick.net\", \"googledomains.com\",\n",
    "                        \"firebaseio.com\", \"firebaseapp.com\", \"notion.so\", \"notion-static.com\",\n",
    "                        \"netlify.app\", \"vercel.app\", \"cloudinary.com\", \"res.cloudinary.com\"\n",
    "                        ]\n",
    "def get_suspicious_keywords():\n",
    "     return [\"login\", \"signin\", \"verify\", \"auth\", \"password\", \"2fa\", \"secure\"]\n",
    "\n",
    "from re import error\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from tldextract import extract\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import logging\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from multiprocessing import Process\n",
    "\n",
    "SUSPICIOUS_WORDS_REGEX = re.compile(\n",
    "    r\"(log[\\s\\-]?in|sign[\\s\\-]?in|auth|user(name)?|email|phone|account|\"\n",
    "    r\"credential|password|passcode|pin|security[\\s\\-]?code|credit[\\s\\-]?card|cvv|expiry|iban|bank)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "lEGIT=1\n",
    "sUS=0\n",
    "pHISHING=-1\n",
    "data = {\n",
    "    \"URL\": [\n",
    "       \"https://heatmap.com\",\"https://gamepedia.com\"\n",
    "       ,\"https://9xflix.pink\",\"https://redditblog.com\"\n",
    "    ],\n",
    "    \"validity\": ['safe', 'safe', 'safe', 'safe']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"/content/test_urls.csv\", index=False)\n",
    "print(\"âœ… Created small test CSV!\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def element_extraction_from_html(soup_html: BeautifulSoup, tag: str = None, attribute=None) -> list:\n",
    "    try:\n",
    "        if not tag:\n",
    "            return []\n",
    "        if attribute:\n",
    "            return soup_html.find_all(tag, **{attribute: True})\n",
    "        else:\n",
    "            return soup_html.find_all(tag)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed to extract elements for tag={tag} attribute={attribute}: {e}\")\n",
    "        return []\n",
    "\n",
    "def normalize_domain(url: str):\n",
    "    try:\n",
    "        parts_of_url = extract(url)\n",
    "        if not (parts_of_url.domain and parts_of_url.suffix):\n",
    "            return None\n",
    "        domain = f\"{parts_of_url.domain}.{parts_of_url.suffix}\"\n",
    "        return domain\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed to normalize domain for url={url}: {e}\")\n",
    "        return None\n",
    "\n",
    "''' specific  function: check the html element that competable with each tag that indicates for a phishing site '''\n",
    "\n",
    "def safe_extract(tag, attribute):\n",
    "    try:\n",
    "        if tag and hasattr(tag, 'get'):\n",
    "            return tag.get(attribute, \"\").strip()\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "#####################1-favicon#############################\n",
    "def favicon_check(link_tag: list, base_domain: str) -> int:\n",
    "    try:\n",
    "        icon_links = []\n",
    "        for link in link_tag:\n",
    "            try:\n",
    "                rel = link.get(\"rel\", [])\n",
    "                if any(\"icon\" in r.lower() for r in rel):\n",
    "                    icon_links.append(link)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem reading link rel attribute: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not icon_links:\n",
    "            return lEGIT\n",
    "\n",
    "        for link in icon_links:\n",
    "            try:\n",
    "                href = link.get(\"href\", \"\")\n",
    "                parsed = normalize_domain(href)\n",
    "                if not parsed:\n",
    "                    continue\n",
    "\n",
    "                netloc = parsed.lower()\n",
    "                if any(safe in netloc for safe in get_known_favicon_hosts()):\n",
    "                    continue\n",
    "\n",
    "                expected_domain = base_domain.lower()\n",
    "\n",
    "                if not (netloc == expected_domain):\n",
    "                    return pHISHING\n",
    "\n",
    "                if not (href.endswith(\".ico\") or href.endswith(\".png\") or href.endswith(\".gif\")):\n",
    "                    return pHISHING\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing favicon link: {e}\")\n",
    "                continue\n",
    "\n",
    "        return lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in favicon_check: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################2-anchor#############################\n",
    "def extract_url_of_anchor_feature(a_list: list, base_domain) -> int:\n",
    "    try:\n",
    "        total = int(len(a_list))\n",
    "        phishy_count = 0\n",
    "\n",
    "        for tag in a_list:\n",
    "            try:\n",
    "                href = safe_extract(tag, \"href\")\n",
    "\n",
    "                if (not href) or href.strip() in [\"#\", \"javascript:void(0);\", \"javascript:\"]:\n",
    "                    phishy_count += 1\n",
    "                    continue\n",
    "\n",
    "                href_domain = normalize_domain(href)\n",
    "                if not href_domain:\n",
    "                    continue\n",
    "                if href_domain != base_domain:\n",
    "                    phishy_count += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing anchor tag: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total == 0:\n",
    "            return lEGIT\n",
    "\n",
    "        ratio = phishy_count / total\n",
    "        if ratio > stage_3_config_dict[\"url_of_anchor_upper_tresh\"]:\n",
    "            return pHISHING\n",
    "        elif stage_3_config_dict[\"url_of_anchor_lower_tresh\"] <= ratio <= stage_3_config_dict[\"url_of_anchor_upper_tresh\"]:\n",
    "            return sUS\n",
    "        else:\n",
    "            return lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_url_of_anchor_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################3-link_count#############################\n",
    "def link_count_in_html(total_link_list: list, base_domain: str) -> int:\n",
    "    try:\n",
    "        total = len(total_link_list)\n",
    "        if not total:\n",
    "            return lEGIT\n",
    "\n",
    "        phishy_count = 0\n",
    "\n",
    "        for tag in total_link_list:\n",
    "            try:\n",
    "                tag_name = tag.name.lower()\n",
    "\n",
    "                if tag_name == \"script\":\n",
    "                    value = safe_extract(tag, \"src\")\n",
    "                elif tag_name == \"meta\":\n",
    "                    value = safe_extract(tag, \"content\")\n",
    "                else:\n",
    "                    value = safe_extract(tag, \"href\")\n",
    "\n",
    "                if not value:\n",
    "                    continue\n",
    "\n",
    "                domain = normalize_domain(value)\n",
    "                if not domain:\n",
    "                    continue\n",
    "\n",
    "                if domain != base_domain:\n",
    "                    phishy_count += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing link tag: {e}\")\n",
    "                continue\n",
    "\n",
    "        ratio = phishy_count / total\n",
    "        if ratio > stage_3_config_dict[\"link_count_html_upper_tresh\"]:\n",
    "            return pHISHING\n",
    "        elif stage_3_config_dict[\"link_count_html_lower_tresh\"] < ratio <= stage_3_config_dict[\"link_count_html_upper_tresh\"]:\n",
    "            return sUS\n",
    "        else:\n",
    "            return lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in link_count_in_html: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################4-request_url#############################\n",
    "def extract_request_url_feature(elements: list, base_domain: str) -> int:\n",
    "    try:\n",
    "        total = len(elements)\n",
    "        if total == 0:\n",
    "            return lEGIT\n",
    "\n",
    "        external_count = 0\n",
    "        for tag in elements:\n",
    "            try:\n",
    "                src = safe_extract(tag, \"src\")\n",
    "                if not src:\n",
    "                    continue\n",
    "\n",
    "                domain = normalize_domain(src)\n",
    "                if not domain:\n",
    "                    continue\n",
    "\n",
    "                if domain != base_domain:\n",
    "                    external_count += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing request URL tag: {e}\")\n",
    "                continue\n",
    "\n",
    "        ratio = external_count / total\n",
    "        if ratio >= stage_3_config_dict[\"request_url_upper_tresh\"]:\n",
    "            return pHISHING\n",
    "        else:\n",
    "            return lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_request_url_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "#####################5-sfh#############################\n",
    "def extract_server_form_handler_feature(form_list: list, base_domain: str) -> int:\n",
    "    try:\n",
    "        total_score = 0\n",
    "        for form in form_list:\n",
    "            form_score = 0\n",
    "            try:\n",
    "                action = safe_extract(form, \"action\")\n",
    "                if not action or action.lower() in [\"about:blank\", \"#\", \"\"]:\n",
    "                    form_score += 2\n",
    "                else:\n",
    "                    action_domain = normalize_domain(action)\n",
    "                    if action_domain and action_domain != base_domain:\n",
    "                        form_score += 4\n",
    "\n",
    "                inputs = form.find_all(\"input\") if hasattr(form, 'find_all') else []\n",
    "                for i in inputs:\n",
    "                    try:\n",
    "                        t = i.get(\"type\", \"\").lower()\n",
    "                        n = i.get(\"name\", \"\").lower()\n",
    "                        if t == \"password\":\n",
    "                            form_score += 2\n",
    "                        if any(w in n for w in get_suspicious_keywords()):\n",
    "                            form_score += 1\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"[ERROR] Problem analyzing input field: {e}\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing form element: {e}\")\n",
    "                continue\n",
    "\n",
    "            total_score += form_score\n",
    "\n",
    "        if total_score >= 5:\n",
    "            return pHISHING\n",
    "        elif total_score >= 2:\n",
    "            return sUS\n",
    "        else:\n",
    "            return lEGIT\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_server_form_handler_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################6-iframe#############################\n",
    "def extract_iframe_feature(iframe_list: list, base_domain: str) -> int:\n",
    "    if not iframe_list:\n",
    "        return lEGIT\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    try:\n",
    "        for iframe in iframe_list:\n",
    "            try:\n",
    "                score = 0\n",
    "                src = iframe.get(\"src\", \"\").strip().lower()\n",
    "                srcdoc = iframe.get(\"srcdoc\", \"\").strip().lower()\n",
    "                style = iframe.get(\"style\", \"\").lower()\n",
    "                width = iframe.get(\"width\", \"\").strip()\n",
    "                height = iframe.get(\"height\", \"\").strip()\n",
    "                iframe_domain = normalize_domain(src) if src else \"\"\n",
    "\n",
    "                is_external = iframe_domain and iframe_domain != base_domain\n",
    "\n",
    "                if any(x in src for x in [\"ads\", \"analytics\", \"pixel\", \"tracker\", \"doubleclick\"]):\n",
    "                    continue\n",
    "\n",
    "                if \"display:none\" in style or \"visibility:hidden\" in style:\n",
    "                    score += 3\n",
    "\n",
    "                if width == \"0\" or height == \"0\":\n",
    "                    score += 2\n",
    "\n",
    "                if is_external:\n",
    "                    score += 2\n",
    "\n",
    "                if hasattr(iframe, 'has_attr') and not iframe.has_attr(\"sandbox\"):\n",
    "                    score += 1\n",
    "\n",
    "                if srcdoc:\n",
    "                    try:\n",
    "                        from bs4 import BeautifulSoup\n",
    "                        clean_srcdoc_text = BeautifulSoup(srcdoc, \"html.parser\").get_text().lower()\n",
    "\n",
    "                        if SUSPICIOUS_WORDS_REGEX.search(clean_srcdoc_text):\n",
    "                            score += 3\n",
    "                        if \"<script\" in srcdoc or \"javascript:\" in srcdoc:\n",
    "                            score += 3\n",
    "                        if \"display:none\" in srcdoc or \"visibility:hidden\" in srcdoc:\n",
    "                            score += 2\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"[ERROR] Problem parsing srcdoc in iframe: {e}\")\n",
    "                        continue\n",
    "\n",
    "                total_score += score\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing iframe tag: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total_score >= stage_3_config_dict[\"iframe_upper_tresh\"]:\n",
    "            return pHISHING\n",
    "        elif stage_3_config_dict[\"iframe_lower_tresh\"] <= total_score < stage_3_config_dict[\"iframe_upper_tresh\"]:\n",
    "            return sUS\n",
    "        else:\n",
    "            return lEGIT\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_iframe_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "#####################7-suspicious_js#############################\n",
    "### need to improve this function-js behavior#####!!!!!!!!!!!!!\n",
    "\n",
    "def detect_suspicious_js_behavior(soup: BeautifulSoup, base_domain: str) -> int:\n",
    "    score = 0\n",
    "\n",
    "    try:\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_iframe_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "    for script in inline_scripts:\n",
    "        try:\n",
    "            content = script.get_text().strip().lower()\n",
    "        except Exception:\n",
    "            content=\"\"\n",
    "\n",
    "        for pattern in get_high_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                score += 3\n",
    "\n",
    "        for pattern in get_medium_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                score += 2\n",
    "\n",
    "        for pattern in get_low_risk_patterns():\n",
    "            if re.search(pattern, content):\n",
    "                score += 1\n",
    "\n",
    "    try:\n",
    "        external_scripts = soup.find_all(\"script\", src=True)\n",
    "        for script in external_scripts:\n",
    "            src = safe_extract(script, \"src\")\n",
    "            domain = normalize_domain(src)\n",
    "            if domain and domain != base_domain and domain not in get_known_safe_script_hosts():\n",
    "                score += 1\n",
    "\n",
    "        if score >= stage_3_config_dict[\"js_upper_tresh\"]:\n",
    "            return pHISHING\n",
    "        elif score >= stage_3_config_dict[\"js_lower_tresh\"]:\n",
    "            return sUS\n",
    "        else:\n",
    "            return lEGIT\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_iframe_feature: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################8-nlp#############################\n",
    "def nlp_based_phishing_text_check(soup: BeautifulSoup) -> int:\n",
    "    try:\n",
    "         text = soup.get_text(strip=True).lower()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in extract_iframe_feature: {e}\")\n",
    "        return sUS\n",
    "    matches = SUSPICIOUS_WORDS_REGEX.findall(text)\n",
    "\n",
    "    if not matches:\n",
    "        return lEGIT\n",
    "\n",
    "    total_words = len(text.split())\n",
    "    match_ratio = len(matches) / total_words if total_words > 0 else 0\n",
    "\n",
    "    if match_ratio > stage_3_config_dict[\"nlp_upper_tresh\"]:\n",
    "        return pHISHING\n",
    "    elif match_ratio > stage_3_config_dict[\"nlp_lower_tresh\"]:\n",
    "        return sUS\n",
    "    else:\n",
    "        return lEGIT\n",
    "\n",
    "\n",
    "\n",
    "#####################9-analyze_textual_tags#############################\n",
    "def analyze_textual_tags(soup: BeautifulSoup) -> int:\n",
    "    try:\n",
    "        tags = soup.find_all([\"meta\", \"script\"])\n",
    "        texts = []\n",
    "\n",
    "        for t in tags:\n",
    "            try:\n",
    "                content = t.get(\"content\", \"\")\n",
    "                string = t.string or \"\"\n",
    "                combined = (content + \" \" + string).strip()\n",
    "                if combined:\n",
    "                    texts.append(combined)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"[ERROR] Problem analyzing meta/script tag: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not texts:\n",
    "            return lEGIT\n",
    "\n",
    "        full_text = \" \".join(texts).lower()\n",
    "        matches = SUSPICIOUS_WORDS_REGEX.findall(full_text)\n",
    "        total_words = len(full_text.split())\n",
    "\n",
    "        if total_words == 0:\n",
    "            return lEGIT\n",
    "\n",
    "        match_ratio = len(matches) / total_words\n",
    "\n",
    "        if total_words < 50:\n",
    "            if match_ratio > 0.05:\n",
    "                return pHISHING\n",
    "            elif match_ratio > 0.015:\n",
    "                return sUS\n",
    "            else:\n",
    "                return lEGIT\n",
    "        else:\n",
    "            if match_ratio > 0.03:\n",
    "                return pHISHING\n",
    "            elif match_ratio > 0.01:\n",
    "                return sUS\n",
    "            else:\n",
    "                return lEGIT\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in analyze_textual_tags: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################10-dynamic_script#############################\n",
    "def detect_dynamic_script_injection(driver: webdriver) -> int:\n",
    "    try:\n",
    "        injected_scripts = driver.execute_script(\"\"\"\n",
    "            return [...document.scripts].filter(s => s.src || s.innerText.length > 0).length;\n",
    "        \"\"\")\n",
    "        if injected_scripts > 10:\n",
    "            return pHISHING\n",
    "        elif injected_scripts > 5:\n",
    "            return sUS\n",
    "        return lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in analyze_textual_tags: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################11-auto_redirect#############################\n",
    "############## after all html was loaded\n",
    "# this function is blocking because the waiting for full upload of the page ,\n",
    "def detect_auto_redirect(driver: webdriver, base_domain: str, timeout: float = 3.0) -> int:\n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "\n",
    "        page_source = driver.page_source.lower()\n",
    "\n",
    "        if re.search(r'<meta\\s+http-equiv\\s*=\\s*[\"\\']?refresh[\"\\']?', page_source, re.IGNORECASE):\n",
    "            return pHISHING\n",
    "\n",
    "        if re.search(r'(window\\.)?location\\.(href|replace)', page_source):\n",
    "            return pHISHING\n",
    "\n",
    "        final_url = driver.current_url\n",
    "        if not final_url:\n",
    "            return sUS\n",
    "\n",
    "        if normalize_domain(final_url) != base_domain:\n",
    "            return pHISHING\n",
    "\n",
    "        return lEGIT\n",
    "\n",
    "    except Exception:\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################12-login_form_visibility#############################\n",
    "def check_login_form_visibility(driver: webdriver) -> int:\n",
    "    try:\n",
    "        script = \"\"\"\n",
    "        var forms = document.getElementsByTagName('form');\n",
    "        for (var i = 0; i < forms.length; i++) {\n",
    "            var style = window.getComputedStyle(forms[i]);\n",
    "            if (style.display === 'none' || style.visibility === 'hidden' ||\n",
    "                forms[i].offsetWidth === 0 || forms[i].offsetHeight === 0) {\n",
    "                return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "        \"\"\"\n",
    "        hidden = driver.execute_script(script)\n",
    "        return pHISHING if hidden else lEGIT\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] Failed in analyze_textual_tags: {e}\")\n",
    "        return sUS\n",
    "\n",
    "\n",
    "#####################13-onmouseover#############################\n",
    "def detect_onmouseover_in_dom(soup: BeautifulSoup) -> int:\n",
    "    try:\n",
    "        tags_with_onmouseover = soup.find_all(attrs={\"onmouseover\": True})\n",
    "\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "        suspicious_script = any(\"onmouseover\" in (script.string or \"\").lower() for script in inline_scripts)\n",
    "\n",
    "        if tags_with_onmouseover or suspicious_script:\n",
    "            return pHISHING\n",
    "        else:\n",
    "            return lEGIT\n",
    "    except Exception:\n",
    "        return sUS\n",
    "\n",
    "\n",
    "\n",
    "#####################14-right_click_block#############################\n",
    "def detect_right_click_block(soup: BeautifulSoup) -> int:\n",
    "    try:\n",
    "        contextmenu_tags = soup.find_all(attrs={\"oncontextmenu\": True})\n",
    "        inline_scripts = soup.find_all(\"script\", src=False)\n",
    "        suspicious_script = False\n",
    "        for script in inline_scripts:\n",
    "            content = (script.string or \"\").lower()\n",
    "\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            if re.search(r\"event\\s*\\.\\s*button\\s*==\\s*2\", content) or \"contextmenu\" in content:\n",
    "                suspicious_script = True\n",
    "                break\n",
    "\n",
    "        if contextmenu_tags or suspicious_script:\n",
    "            return pHISHING\n",
    "        else:\n",
    "            return lEGIT\n",
    "\n",
    "    except Exception:\n",
    "        return sUS\n",
    "\n",
    "###########################end###########################################\n",
    "\n",
    "\n",
    "def find_html_features(html:BeautifulSoup, url: str, feature_type: str,driver:webdriver):\n",
    "    domain = normalize_domain(url)\n",
    "    str_html = str(html)\n",
    "    elements=[]\n",
    "    if feature_type == \"favicon_check\":\n",
    "        elements = element_extraction_from_html(html, tag=\"link\", attribute=\"href\")\n",
    "        return favicon_check(elements, domain)\n",
    "    elif feature_type == \"url_anchor\":\n",
    "        elements = element_extraction_from_html(html, tag=\"a\", attribute=\"href\")\n",
    "        return extract_url_of_anchor_feature(elements,domain)\n",
    "    elif feature_type == \"links_in_tags\":\n",
    "        elements+= element_extraction_from_html(html, tag=\"meta\", attribute=\"content\")\n",
    "        elements += element_extraction_from_html(html, tag=\"script\", attribute=\"src\")\n",
    "        elements += element_extraction_from_html(html, tag=\"link\", attribute=\"href\")\n",
    "        return link_count_in_html(elements,domain)\n",
    "    elif feature_type== \"request_sources_from_diff_url\":\n",
    "        for tag in [\"img\",\"source\",\"audio\",\"video\",\"embed\",\"iframe\"]:\n",
    "            elements+= element_extraction_from_html(html, tag=tag, attribute=\"src\")\n",
    "        return extract_request_url_feature(elements,domain)\n",
    "    elif feature_type == \"sfh\":\n",
    "        elements = element_extraction_from_html(html, tag=\"form\", attribute=\"action\")\n",
    "        elements += element_extraction_from_html(html, tag=\"form\")\n",
    "        return extract_server_form_handler_feature(elements, domain)\n",
    "    elif feature_type==\"iframe\":\n",
    "        elements+=element_extraction_from_html(html,tag=\"iframe\")\n",
    "        return extract_iframe_feature(elements,domain)\n",
    "    elif feature_type == \"suspicious_js\":\n",
    "        return detect_suspicious_js_behavior(html,domain)\n",
    "    elif feature_type ==\"nlp_text\":\n",
    "       return nlp_based_phishing_text_check(html)\n",
    "    elif feature_type ==\"analyze_textual_tags\":\n",
    "        return  analyze_textual_tags(html)\n",
    "    elif feature_type ==\"detect_dynamic_script_injection\":\n",
    "        return detect_dynamic_script_injection(driver)\n",
    "    elif feature_type == \"detect_auto_redirect\":\n",
    "        return detect_auto_redirect(driver,domain)\n",
    "    elif feature_type == \"check_login_form_visibility\":\n",
    "        return check_login_form_visibility(driver)\n",
    "    elif feature_type == \"detect_onmouseover_in_dom\":\n",
    "        return detect_onmouseover_in_dom(html)\n",
    "    elif feature_type == \"detect_right_click_block\":\n",
    "        return  detect_right_click_block(html)\n",
    "    return\n",
    "\n",
    "\n",
    "def wait_for_initial_html(driver, min_length=500, timeout=5.0) -> str:\n",
    "    \"\"\"\n",
    "    Waits until there is enough initial HTML content.\n",
    "    Returns the HTML once ready, or the best available after timeout.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            ready_state = driver.execute_script(\"return document.readyState;\")\n",
    "            if ready_state not in [\"interactive\", \"complete\"]:\n",
    "                time.sleep(0.05)\n",
    "                continue\n",
    "\n",
    "            html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "            if html and len(html) >= min_length:\n",
    "                return html\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    # fallback\n",
    "    try:\n",
    "        html = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "    except Exception:\n",
    "        html = \"\"\n",
    "    return html\n",
    "\n",
    "def test_headless_browser_firefox(url: str, mode: str = \"full\") -> tuple:\n",
    "    \"\"\"\n",
    "    mode: \"initial\" -> fetch initial enough HTML\n",
    "          \"full\"    -> wait for fully loaded HTML\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "\n",
    "        ffx_options = Options()\n",
    "        ffx_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Firefox(options=ffx_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        if mode == \"full\":\n",
    "\n",
    "            # ğŸ”µ ××¦×‘ FULL â†’ ×œ×—×›×•×ª ×©×”×“×£ ×™×˜×¢×Ÿ ×œ×’××¨×™\n",
    "            WebDriverWait(driver, 10).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            html = driver.page_source\n",
    "        elif mode == \"initial\":\n",
    "            # ğŸŸ¢ ××¦×‘ INITIAL â†’ ×œ×—×›×•×ª ×©×™×”×™×” ××¡×¤×™×§ HTML ×¨××©×•× ×™\n",
    "            html = wait_for_initial_html(driver, min_length=500, timeout=5.0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode. Use 'initial' or 'full'.\")\n",
    "\n",
    "        if not html or html.strip() == \"\":\n",
    "            logging.error(f\"[ERROR] Empty page source for {url}\")\n",
    "            return None, None\n",
    "\n",
    "        return html, driver\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ [ERROR] Failed to load {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    finally:\n",
    "        if driver is not None:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# Set logging level to DEBUG\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "def extract_stage3_features_debug(input_csv_path, output_csv_path, id):\n",
    "    driver = None\n",
    "    results = []\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        total = len(df)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            url = row['URL']\n",
    "            label = row.get('validity', 0)\n",
    "            print(f\"{id}:\\nğŸ” [{index+1}/{total}] Processing URL: \\033[96m{url}\\033[0m\")\n",
    "\n",
    "            html, driver = test_headless_browser_firefox(url,\"full\")  #change here to \"full\" or \"initial\"\n",
    "            if html is None or driver is None:\n",
    "                logging.error(f\"[ERROR] Failed to load page: {url}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            features = [url]\n",
    "\n",
    "            for feature_type in [\n",
    "                \"favicon_check\",\n",
    "                \"url_anchor\",\n",
    "                \"links_in_tags\",\n",
    "                \"request_sources_from_diff_url\",\n",
    "                \"sfh\",\n",
    "                \"iframe\",\n",
    "                \"suspicious_js\",\n",
    "                \"nlp_text\",\n",
    "                \"analyze_textual_tags\",\n",
    "                \"detect_dynamic_script_injection\",\n",
    "                \"detect_auto_redirect\",\n",
    "                \"check_login_form_visibility\",\n",
    "                \"detect_onmouseover_in_dom\",\n",
    "                \"detect_right_click_block\"\n",
    "            ]:\n",
    "                try:\n",
    "                    result = find_html_features(soup, url, feature_type, driver)\n",
    "                    logging.info(f\"{feature_type:35} â†’ {result}\")\n",
    "                    features.append(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"[ERROR] {feature_type} failed for {url} â†’ {e}\")\n",
    "                    features.append(-999)\n",
    "\n",
    "            features.append(label)\n",
    "            results.append(features)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[ERROR] General failure in extract_stage3_features_debug: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if driver is not None:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Define headers\n",
    "        headers = [\n",
    "            \"url\", \"favicon_check\", \"url_anchor\", \"links_in_tags\",\n",
    "            \"request_sources_from_diff_url\", \"sfh\", \"iframe\",\n",
    "            \"suspicious_js\", \"nlp_text\", \"analyze_textual_tags\",\n",
    "            \"detect_dynamic_script_injection\", \"detect_auto_redirect\",\n",
    "            \"check_login_form_visibility\", \"detect_onmouseover_in_dom\",\n",
    "            \"detect_right_click_block\", \"label\"\n",
    "        ]\n",
    "\n",
    "        # Save to CSV\n",
    "        try:\n",
    "            pd.DataFrame(results, columns=headers).to_csv(output_csv_path, index=False)\n",
    "            logging.info(f\"\\nâœ… Finished. CSV saved to: \\033[92m{output_csv_path}\\033[0m\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[ERROR] Failed saving CSV {output_csv_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_single_file(input_csv_path, output_csv_path,pid):\n",
    "    try:\n",
    "        logging.info(f\"ğŸš€ Starting pid= {pid} for {input_csv_path}\")\n",
    "        extract_stage3_features_debug(input_csv_path, output_csv_path,pid)\n",
    "        logging.info(f\"âœ… Finished pid={pid} for {input_csv_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ Error in pid={pid} processing {input_csv_path}: {e}\")\n",
    "\n",
    "def main_parallel():\n",
    "\n",
    "    files = [\n",
    "        (\"/content/phish_urls_part_1.csv\", \"/content/phish_urls_part_1.csv\",\"1\"),\n",
    "        (\"/content/phish_urls_part_2.csv\", \"/content/phish_urls_part_2.csv\",\"2\"),\n",
    "        (\"/content/phish_urls_part_3.csv\", \"/content/phish_urls_part_3.csv\",\"3\"),\n",
    "         (\"/content/phish_urls_part_4.csv\", \"/content/phish_urls_part_4.csv\",\"4\")\n",
    "    ]\n",
    "\n",
    "    total_files = len(files)\n",
    "    print(f\"ğŸš€ Starting sequential processing of {total_files} files...\\n\")\n",
    "\n",
    "    for idx, (input_csv, output_csv, id) in enumerate(files, 1):\n",
    "        print(f\"ğŸ”µ [{idx}/{total_files}] Processing file ID={id}\")\n",
    "        try:\n",
    "            process_single_file(input_csv, output_csv, id)\n",
    "            print(f\"âœ… Finished processing file ID={id}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing file ID={id}: {e}\\n\")\n",
    "\n",
    "    print(\"ğŸ¯ All files processed successfully!\\n\")\n",
    "\n",
    "main_parallel()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFLF0A00PLaP",
    "outputId": "a133532a-e75f-473b-8ba0-08e1e59f5cd0"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
